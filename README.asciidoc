// begin header
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
endif::[]
:numbered:
:toc: macro
:toc-title: pass:[<b>Table of Contents</b>]
// end header

= Ansible-tower-configuration

Repo for bootstrapping Ansible Tower instances.

toc::[]

== Credentials repository


This project uses an https://github.com/integr8ly/tower_dummy_credentials[external credentials repository] as it's inventory source which also includes all of the variables required and the password to be used for the running of the playbooks in the project.

Once the https://github.com/integr8ly/tower_dummy_credentials[external credentials repository] has been bootstrapped with the required variables to suit your own environment, it should be used as the inventory source for executing playbooks in this project, replacing `<path-to-local-credentials-project>` with the path to your local credentials project.

== Building Images

=== Integreatly Ansible Tower Base Image

To build and tag the Integreatly Ansible Tower Base docker image simply run:

```bash
cd images/tower_base/ && make
```

To push the built image to quay.io run:

```bash
make image/push
```

=== Ansible Tower Bootstrap Image

To build and tag the Ansible Tower Bootstrap docker image simply run:

```bash
cd images/tower_bootstrap/ && make
```

To push the built image to quay.io run:

```bash
make image/push
```

== Ansible Tower Installation

The `install_tower.yml` playbook will install Ansible Tower on a target Openshift cluster. The playbook requires the target tower environment to be specified.

* `tower_openshift_master_url`: The URL of the target Openshift cluster
* `tower_openshift_username`: Cluster admin user on target Openshift cluster
* `tower_openshift_password`: Password of cluster admin user on target Openshift Cluster

```bash
ansible-playbook -i <path-to-local-credentials-project>/inventories/hosts playbooks/install_tower.yml -e tower_openshift_master_url=<tower_openshift_master_url> -e tower_openshift_username=<tower_openshift_cluster_admin_username> -e tower_openshift_password=<tower_openshift_cluster_admin_password> -e tower_openshift_pg_pvc_size=10Gi --ask-vault-pass
```

A number of default values are used when installing Ansible Tower on the target Openshift cluster, any of which can be overridden with the use of environmental variables. These default values include several password values which are assigned a default value of `CHANGEME`, as can be seen below.

* `tower_openshift_project`: The name of the newly created Openshift project (default project name is `tower`)
* `tower_version`: The version of the Ansible Tower Openshift setup project to install (default version is `3.4.3`)
* `tower_archive_url`: The URL of the Ansible Tower Openshift installation project archive file to be used (default URL is `https://releases.ansible.com/ansible-tower/setup_openshift/<tower_version>`)
* `tower_admin_user`: The username required to login to the newly installed Tower instance (default username is `admin`)
* `tower_admin_password`: The password required to login to the newly installed Tower instance (default password is `CHANGEME`)
* `tower_rabbitmq_password`: The password required to login to RabbitMQ (default password is `CHANGEME`)
* `tower_pg_password`: The password required to login to PostgreSQL (default password is `CHANGEME`)
* `tower_openshift_pg_pvc_size`: Size of Postgres persistent volume. Defaults to `100Gi` which is recommended for production environments

=== Tower Host Name Update

Once the new Tower instance has successfully been installed on the target Openshift cluster, the host name of the new Tower instance must be placed into the `tower_host` variable value, which is located in the `<ENVIRONMENT>_tower_credentials_list.yml` file in the `external_credentials_repo` project.

```bash
<env>_tower_host: 'tower.example.com'
```

== Bootstrapping

=== Tower Bootstrapping

The `bootstrap.yml` playbook will run the Prerequisite, Integreatly, Cluster Create, Cluster Teardown, Validation and Notification bootstrap playbooks in succession. These individual playbooks can be run independently if required, with instructions on how to do so in the following sections. The playbook requires the target tower environment to be specified.

* `tower_environment`: The Ansible Tower environment (dev/test/qe etc.)

```bash
ansible-playbook -i <path-to-local-credentials-project>/inventories/hosts playbooks/bootstrap.yml -e tower_environment=<tower-environment> --ask-vault-pass
```

=== Openshift Dedicated

If you also wish to bootstrap the tower instance with the OSD integreatly install workflow you need to run this play after running the bootstrap.yml play.

```bash
ansible-playbook -i <path-to-local-credentials-project>/inventories/hosts playbooks/bootstrap_osd_integreatly_install.yml -e tower_environment=<tower-environment> --ask-vault-pass
```

This will create the resources necessary to use the *Integreatly_Bootstrap_and_install_[OSD]* workflow which will install Integreatly on a targeted OSD cluster.

=== Prerequisite Bootstrapping

Prior to running any jobs stored in this repository, the target Ansible tower instance must first be bootstrapped with some generic resources. The playbook requires the target tower environment to be specified.

* `tower_environment`: The Ansible Tower environment (dev/test/qe)

```bash
ansible-playbook -i <path-to-local-credentials-project>/inventories/hosts playbooks/bootstrap_tower.yml -e tower_environment=<tower-environment>
```

=== Integreatly Bootstrapping

The `bootstrap_integreatly.yml` playbook will bootstrap a target Ansible Tower instance with all resources required to execute a workflow that allows end users to install Integreatly against a specified Openshift cluster. Note: This is currently limited to clusters that are provisioned via the Tower cluster provision workflow.

There are no additional parameters required by default:

```bash
ansible-playbook -i <path-to-local-credentials-project>/hosts playbooks/bootstrap_integreatly.yml --ask-vault-pass
```

=== Integreatly Install

Following the bootstrapping of Integreatly resources, a new workflow named `Integreatly Install Workflow` should be available from the Tower console:

The workflow requires the following parameters to be specified before running:

* `Cluster Name`: The name/ID of the Openshift cluster to target
* `Openshift Master URL`: The Public URL of the Openshift Master
* `Cluster Admin Username`: The username of a cluster-admin account on the target Openshift cluster
* `Cluster Admin Password`: The password of the cluster admin user account specified
* `GIT URL`: The URL of the target Integreatly installer Git repository
* `GIT Ref`: Git reference for Integreatly installer repository
* `User Count`: The number of users to pre-seed the Integreatly environment with
* `Self Signed Certs`: Set to `false` by default. Set to `true` if the target Openshift cluster uses self signed certificates

=== Integreatly Uninstall

Following the bootstrapping of Integreatly resources, a new workflow named `Integreatly Uninstall Workflow` should be available from the Tower console:

The workflow requires the following parameters to be specified before running:

* `Cluster Name`: The name/ID of the Openshift cluster to target
* `Openshift Master URL`: The Public URL of the Openshift Master
* `Cluster Admin Username`: The username of a cluster-admin account on the target Openshift cluster
* `Cluster Admin Password`: The password of the cluster admin user account specified
* `GIT URL`: The URL of the target Integreatly installer Git repository
* `GIT Ref`: Git reference for Integreatly installer repository

=== Cluster Create

Once the tower bootstrapping has been run you can bootstrap the cluster create resources. To create all the resources necessary to run a cluster create you must run the `bootstrap_cluster_create.yml` playbook. The playbook doesn't take any extra variables so the command to run is:

```bash
ansible-playbook -i <path-to-local-credentials-project>/hosts playbooks/bootstrap_cluster_create.yml --ask-vault-pass
```

Once the cluster provision resources have been bootstrapped, a new workflow named `Provision Cluster` should be available from the Tower console:

The workflow requires the following parameters to be specified before running:

* `Cluster Name`: The name/ID of the Openshift cluster to target
* `AWS Region`: The region to create the cluster in
* `Domain Name`: The domain name to be used to create the cluster
* `AWS Account Name`: The name of the AWS account to be used to create the cluster

=== Cluster Deprovision

Once the tower bootstrapping has been run you can bootstrap the cluster deprovision resources. To create all the resources necessary to run a cluster deprovision you must run the `bootstrap_cluster_teardown.yml` playbook. The playbook doesn't take any extra variables so the command to run is:

```bash
ansible-playbook -i <path-to-local-credentials-project>/hosts playbooks/bootstrap_cluster_teardown.yml -ask-vault-pass
```

Once the cluster deprovision resources have been bootstrapped, a new workflow named `Deprovision Cluster` should be available from the Tower console:

The workflow requires the following parameters to be specified before running:

* `Cluster Name`: The name/ID of the Openshift cluster to target
* `AWS Region`: The region that the cluster resides in
* `Domain Name`: The cluster domain name
* `AWS Account Name`: The name of the AWS account used to create the cluster

=== Validation Bootstrapping

The `bootstrap_validation.yml` playbook will bootstrap the target Ansible Tower instance with the resources required to compare the variables in the supplied repository with the variables in the https://github.com/integr8ly/tower_dummy_credentials[external credentials repository].

```bash
ansible-playbook -i <path-to-local-credentials-project>/hosts playbooks/bootstrap_validation.yml -ask-vault-pass
```

Once the bootstrapping process has completed, a new job template named `Compare Project Variables` will be available, which  requires the following parameters to be specified via the survey before running:

* `Tower Dummy Credentials Repository Branch`: The branch of the tower dummy credentials project
* `Comparison Project Name`: The name of the project to compare against
* `Comparison Project SSH URL`: The Github SSH URL of the project to compare
* `Comparison Project Branch`: The branch of the comparison project to used in the comparison
* `Inventory Path`: The inventory source path of the variables in the comparison project

=== Notifications Bootstrapping

The `bootstrap_notifications.yml` playbook will bootstrap the target Ansible Tower instance with resources required to send an email notification if the https://github.com/integr8ly/ansible-tower-configuration#validation-bootstrapping[validation job] fails. A scheduled job named `Credential Repositories Sync` that runs the `Compare Project Variables` job template is also created, with this scheduled job being disabled default.

```bash
ansible-playbook -i <path-to-local-credentials-project>/hosts playbooks/bootstrap_notifications.yml -ask-vault-pass
```

== Performance Tuning

In order to support a large number of running jobs concurrently on Ansible Tower, it's important to ensure that the necessary resources have been configured.

=== Task Execution Container

All jobs on Tower are run from the Task Execution container named `ansible-tower-celery`. When looking to assign additional resources to Tower jobs, it's this container that needs to be updated with new limits.

By default, the `ansible-tower-celery` container has set limits of `1500` millicores CPU and `2Gi` Memory. To update these limits, edit the `ansible-tower` stateful set and modify existing limits, see example snippet below:

```yaml
      name: ansible-tower-celery
      resources:
        requests:
          cpu: 1500m
          memory: 2Gi
```

For new installations, the default limits can be overridden as part of the install using the below variables:

```yaml
tower_task_mem_request
tower_task_cpu_request
```

NOTE: There is also a limit set for the Tower namespace named `tower-core-resource-limits`. The default values here may need to be updated to match the set values in the steps above.

=== Resource Requests and Request Planning

Ansible Tower is intelligent enough to limit the number of jobs executed based on set limits. These limits are determined using algorithms for both CPU and Memory, see official docs for full details:

https://docs.ansible.com/ansible-tower/3.3.0/html/administration/openshift_configuration.html#resource-requests-and-request-planning

== Automated Testing

This repo is configured to run automated tests using prow when a pr is created.

One of these is an e2e test. If you want to run this test locally before pushing a pr you can do that by taking the below steps.

=== Export the required variables

```
export OPENSHIFT_MASTER=<master-host>
export TOWER_OPENSHIFT_USERNAME=<openshift-user>
export TOWER_OPENSHIFT_PASSWORD=<openshift-password>
export TOWER_LICENSE='<valid-tower-license-with-eula-accepted-value>'
export TOWER_USERNAME=admin
export TOWER_PASSWORD=<tower-password>
```

=== Run the make target

Run the make command

`make test/e2e`

== Contributing

Please open a Github issue for any bugs or problems you encounter.